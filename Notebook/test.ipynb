{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Adam' from 'keras.optimizers' (/Users/danielnguyen/miniforge3/envs/tk-ML/lib/python3.9/site-packages/keras/optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielnguyen/Repo/AI/Notebook/test.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielnguyen/Repo/AI/Notebook/test.ipynb#ch0000000?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlosses\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseCategoricalCrossentropy\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielnguyen/Repo/AI/Notebook/test.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregularizers\u001b[39;00m \u001b[39mimport\u001b[39;00m l2\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danielnguyen/Repo/AI/Notebook/test.ipynb#ch0000000?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielnguyen/Repo/AI/Notebook/test.ipynb#ch0000000?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report, ConfusionMatrixDisplay\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielnguyen/Repo/AI/Notebook/test.ipynb#ch0000000?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msn\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam' from 'keras.optimizers' (/Users/danielnguyen/miniforge3/envs/tk-ML/lib/python3.9/site-packages/keras/optimizers.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, Dense, GlobalAveragePooling2D, \\\n",
    "                         MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv('../gdsc-ai-challenge/trainLabels.csv')\n",
    "\n",
    "class_names = label_df['label'].unique()\n",
    "number_of_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1):\n",
    "    \"\"\"Split the dataset into three subsets: train, validation (dev) and test set.\n",
    "\n",
    "    Returns three tuples, containing each subset with its size.\n",
    "\n",
    "    Keyword arguments:\n",
    "\n",
    "    ds -- tf.data.Dataset object\n",
    "\n",
    "    ds_size -- size of the dataset\n",
    "\n",
    "    train_split -- percentage to split into train set (default to 0.8)\n",
    "\n",
    "    val_split -- percentage to split into validation set (default to 0.1)\n",
    "\n",
    "    test_split -- percentage to split into test set (default to 0.1)\n",
    "    \"\"\"\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return (train_ds, train_size), (val_ds, val_size), (test_ds, ds_size - val_size - train_size)\n",
    "\n",
    "def configure(ds, ds_size, batch_size=32, shuffle=False, augment=False):\n",
    "    \"\"\"Configure the given dataset for better performance (by caching, prefetching and then batching the dataset)\n",
    "\n",
    "    and perform preprocessing to the images in the given dataset.\n",
    "\n",
    "    Returns the optimized dataset.\n",
    "\n",
    "    Keyword arguments:\n",
    "\n",
    "    ds -- tf.data.Dataset object\n",
    "\n",
    "    ds_size -- size of the dataset\n",
    "\n",
    "    batch_size -- size of each batch (default to 32)\n",
    "\n",
    "    shuffle -- whether to shuffle the dataset (default to False)\n",
    "\n",
    "    augment -- whether to perform data augmentation to the dataset (default to False)\n",
    "    \"\"\"\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    rescale = keras.layers.Rescaling(1.0/255)\n",
    "    data_augmentation = keras.Sequential([\n",
    "        keras.layers.RandomFlip('horizontal'),\n",
    "        keras.layers.RandomRotation(0.05, fill_mode='nearest')\n",
    "    ])\n",
    "\n",
    "    ds = ds.map(lambda x, y: (rescale(x), y),\n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=int(ds_size * 0.6))\n",
    "    \n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if augment:\n",
    "        with tf.device('/cpu:0'):\n",
    "            #only perform data augmentation on train set\n",
    "            ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                                    num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 10 classes.\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 12:41:47.972755: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-23 12:41:47.972890: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    '../gdsc-ai-challenge/train',\n",
    "    color_mode='rgb',\n",
    "    batch_size=64,\n",
    "    image_size=(32,32),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ds_size = ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, train_size), (val_ds, val_size), (test_ds, test_size) = split_dataset(ds, \n",
    "                                                                        ds_size, \n",
    "                                                                        train_split=0.7, \n",
    "                                                                        val_split=0.2, \n",
    "                                                                        test_split=0.1)\n",
    "\n",
    "train_ds = configure(train_ds, train_size, augment=True, shuffle=True)\n",
    "val_ds = configure(val_ds, val_size)\n",
    "test_ds = configure(test_ds, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_version(path_to_dir, new_file_name='', new_version=True):\n",
    "    \"\"\"Create a new folder contains new_version weights file\n",
    "\n",
    "    Returns new_version, path to saving folder and path to save new_file_name (preferably .hdf5 files)\n",
    "\n",
    "    Keyword arguments:\n",
    "\n",
    "    path_to_dir -- path to directory where new folder should be created\n",
    "\n",
    "    new_file_name -- name of the desired file (default to '')\n",
    "\n",
    "    new_version -- version of the saved model and new weights file (default to 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert(type(new_version) == bool) , \"new_version must be integer\"\n",
    "\n",
    "    versions_list = sorted([int(version.removeprefix('version')) for version in os.listdir(path_to_dir) if version != \".DS_Store\"])\n",
    "\n",
    "    if new_version == True:\n",
    "        new_version = versions_list[-1] + 1 if len(versions_list) > 0 else 1\n",
    "    else:\n",
    "        new_version = versions_list[-1] if len(versions_list) > 0 else 1\n",
    "\n",
    "    new_version_folder = 'version{}'.format(int(new_version))\n",
    "    new_version_file = new_file_name if len(new_file_name) > 0 else None\n",
    "\n",
    "    save_path = path = os.path.join(path_to_dir, new_version_folder)\n",
    "    if new_version_file is not None:\n",
    "        save_path = os.path.join(save_path, new_version_file)\n",
    "    if versions_list.count(new_version) == 0:\n",
    "        os.makedirs(path)\n",
    "\n",
    "    return new_version, path, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = input(\"\"\"Create new folder?\n",
    "\n",
    "                (Y/n)    \n",
    "                \"\"\")\n",
    "\n",
    "new_version, path, weights_save_path = generate_version('../Model/aiseries', 'weights.best.hdf5', version.lower() == 'y')\n",
    "_, _, report_save_path = generate_version('../TrainingReport', new_version=version.lower() == 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run {path}/model.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "import os\n",
    "\n",
    "def create_model(path_to_weights='', load_weights=True):\n",
    "    \"\"\"Function to create a model\n",
    "\n",
    "    Returns a compiled and optionally loaded model\n",
    "\n",
    "    Keyword arguments:\n",
    "\n",
    "    path_to_weights -- (Optional, only used when load_weights is True) -- Path to weight file (.hdf5 files)\n",
    "\n",
    "    load_weights -- Whether to load weights or not (default to True)\n",
    "    \"\"\"\n",
    "    if (load_weights):\n",
    "        assert(path_to_weights is not None and \n",
    "           os.path.isfile(path_to_weights)), \"path_to_weights must exist and not be empty if load_weights is True, otherwise change load_weights to False\"\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        Input((32,32,3)),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(96, (3,3), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(96, (3,3), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(96, (3,3), padding='same', strides=(2,2),\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(192, (3,3), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(192, (3,3), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(192, (3,3), padding='same', strides=(2,2),\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Conv2D(192, (3,3), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(192, (1,1), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(10, (1,1), padding='same',\n",
    "                            kernel_regularizer=l2(1e-3),\n",
    "#                             activity_regularizer=l2(1e-3),\n",
    "                            kernel_initializer='he_normal',\n",
    "                            activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    if load_weights:\n",
    "        model.load_weights(path_to_weights)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                                 loss='sparse_categorical_crossentropy',\n",
    "                                 metrics=[\n",
    "                                 SparseCategoricalCrossentropy(name='sparse'),\n",
    "                                 'accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(load_weights=False)\n",
    "\n",
    "tf.keras.utils.plot_model(model, os.path.join(path, 'model.png'), show_shapes=True)\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4009867f8b3d0ae5f99565db6746208eea09821b4f82b6b90fecf858e75f24ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tk-ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
