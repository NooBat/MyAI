{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ Multivariate~~Linear~~Regression $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defintion and Usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as univariate linear regression with the only change is instead of one independent variable, we will use **multiple** independent variables (input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_j^{(i)}$ = value of feature $j^{th}$ in the $i^{th}$ training example.<br><br>\n",
    "  $x^{(i)}$ = the input (features) of the $i^{th}$ training example.<br>\n",
    "$~~ m =$ the number of training examples.<br>\n",
    "$~~~ n =$ the number of features.<br>\n",
    "\n",
    "For example:<br>\n",
    "$$ x_1~~x_2~~x_3 $$\n",
    "$$\\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix}$$\n",
    "<br><br>\n",
    "\n",
    "With the matrix above, the rows indicate the number of training examples, the columns indicate the number of features.<br><br>\n",
    "If we say $x^{(2)}$, that means we are taking a row vector $\\begin{bmatrix} 3 & 0 & 1 \\end{bmatrix}$ which is the input of the second training example.<br><br>\n",
    "If we take $x_2^{(3)}$ that means we are referring to third row, second column which is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General form of Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $$\n",
    "\n",
    "Using linear algebra notation:<br>\n",
    "$$h_\\theta(x) = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & ... & \\theta_n \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{bmatrix} = \\theta^T x $$\n",
    "\n",
    "$x^{(i)}_0 = 1$ for $ (i \\in 1,...~,m) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as with one variable. But now we have a set of features. \n",
    "    $$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_j)$$\n",
    "    $$\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}{[(h_\\theta(x^{(i)}) - y^{(i)})  \\phi_j(x^{(i)})]}$$\n",
    "    (simutaneously update $\\theta_j$ for $ j = 0, 1, 2, ..., m$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.<br><br>\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:\n",
    "\n",
    "$$ −1 \\leqslant x_{i} \\leqslant 1 $$\n",
    "or \n",
    "$$ -0.5 \\leqslant x_{i} \\leqslant 0.5 $$\n",
    "\n",
    "The goal is to get all input variables into roughly one of these ranges. This is called **Feature scaling**. In data processing, it is also known as **Data normalization** and is generally performed during the **data processing** step.<br><br>\n",
    "There are two techniques to achieve this:\n",
    "1. **Max - Min normalization**.<br>\n",
    "2. **Mean normalization**.\n",
    "\n",
    "The general formula for Standardization is:\n",
    "    $$ x_i = \\frac{x - \\mu_i}{\\sigma} $$\n",
    "* $\\mu_i$ is the average of all the values for feature $i^{th}$\n",
    "* $\\sigma$ is the standard deviation (which can be found <a href=\"https://en.wikipedia.org/wiki/Standard_deviation\" target=\"_blank\">here</a>), or the range between Max - Min value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Max - Min normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max - Min normalization involves dividing the input values by the ranges of the input variable, resulting in a new range around $1$ .<br>\n",
    "**General Formula** for a Min - Max of $[0, 1]$ is given as:\n",
    "$$x_i = \\frac{x_i - x_{min}}{\\sigma} = \\frac{x_i - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "To rescale between an arbitrary set of values $[a, b]$ , the formula becomes:\n",
    "$$x_i = a + \\frac{(x_i - x_{min})(b - a)}{\\sigma} = a + \\frac{(x_i - x_{min})(b - a)}{x_{max} - x_{min}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mean normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.<br>\n",
    "**General Formula**:\n",
    "$$x_i = \\frac{x_i - \\mu_i}{\\sigma} = \\frac{x_i - \\mu_i}{x_{max} - x_{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Learning rate $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy to choose the most optimal learning rate is by choosing a random value then decrease/increase it accordingly. For example, we first choose the value of $\\alpha = 0.01$. By evaluating the performance of Gradient Descent, we can either increase it by $3$ times ( $0.03$ ) or decrease it by $3$ times ( $0.0003$ ), and so on until the performance is optimal.\n",
    "\n",
    "There are two ways to identify the correct learning rate $\\alpha$ :\n",
    "* **Debugging gradient descent**.\n",
    "* **Automatic convergence test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Debugging gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting cost function $J(\\theta)$ against number of iterations, we can evaluate whether we chose the sufficient $\\alpha$ or not. There are three possible kinds of plot:\n",
    "1. Rises or oscillate back and forth (this happens when $\\alpha$ is too large, making it may not converge).\n",
    "2. Drops gradually (when the $\\alpha$ is small but too small, which leads to the convergence process is slow).\n",
    "3. Sharp decline (this when the $\\alpha$ is sufficiently small, this is the optimal value for learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Automatic convergence test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare convergence if $J(θ)$ decreases by less than E in one iteration, where E is some small value such as $10^{−3}$. However in practice it's difficult to choose this threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special case: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression uses a hypothesis with an n-degree function:\n",
    "$$h(\\theta) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + ... + \\theta_n x^n$$\n",
    "By considering each $\\phi(x)$ as an independent variable, we can see that Polynomial Regression can be treated as a Multivariate Linear Regression with every $x^{(i)}$ = $\\phi_i(x)$ .\n",
    "<br>**Note**: With higher degree functions, we should use feature scaling to reduce the range of each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $~~~~~~~~~~~~~~~~~~~~~~Gradient~~Descent~~~~~~~~~~~~~~~~~~~~$ | $~~~~~~~~~~~~~~~Normal~~equation~~~~~~~~~~~~~~~$ |\n",
    "|:---:|:---:|\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
