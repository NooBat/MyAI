{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Logistic~~Regression$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this is called Regression, it falls under another category of **Supervised Learning** which is **Classification**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification** is a process of categorizing a given set of data into **classes**. It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points. The classes are often referred to as **targets**, **labels** or **categories**.\n",
    "\n",
    "The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which **class**/**category** the new data will fall into.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Given the size of the cancer tumour, we can predict whether this is a benign or malignant tumour.\n",
    "- Given the annual income of an individual, banks can predict whether that individual is capable of paying the debt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the label that we want to predict take on a small number of discrete values. For **Binary Classification**, the label $y$ only take on two values: $0$ (malignant, cannot pay debt) and $1$ (benign, can pay debt) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to use linear regression and map all predictions greater than $0.5$ as $1$ and all less than $0.5$ as $0$ . But this doesn't work well because classification is not actually a linear function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General formula**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want our hypothesis function to give certain discrete values ( $0 \\leqslant f_\\theta(x) \\leqslant 1 $ ), which cannot be achieved with normal linear function $f_\\theta(x) = \\theta^{T} x$ . That is why we are using something called the **Sigmoid function**:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "$$\\Rightarrow f_\\theta(x) = g(\\theta_{T}x) = \\frac{1}{1 + e^{-\\theta_{T}x}}$$\n",
    "This is the reason why **Logistic function** can be used interchangeably with **Sigmoid function**.\n",
    "\n",
    "Here is how the Sigmoid function look like:\n",
    "\n",
    "![Sigmoid function illustration](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/1WFqZHntEead-BJkoDOYOw_2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png?expiry=1638662400000&hmac=8EA0eH0xwVK5nkS2SMV-L8FDqmb3Sc_vHJ-PG5r-gz0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Usage**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to predict the probability that the label $y = 1$ on input $x$ .\n",
    "For example, if:\n",
    "$$x = \\begin{bmatrix} x_0 \\\\ x_1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\text{Tumor Size} \\end{bmatrix} $$\n",
    "$$\\Rightarrow f_\\theta(x) = 0.7 $$\n",
    "Then this tells that there is a 70% chance that the patient's tumor is malignant.\n",
    "\n",
    "Statistically, this could be written as: \n",
    "$$f_\\theta(x) = P(y = 1 | x; \\theta)~~\\text{Meaning: Probability that $y = 1$, given $x$, parameterized by $\\theta$}$$\n",
    "Since there are only two possible outcomes then\n",
    "$$P(y = 0 | x; \\theta) + P(y = 1 | x; \\theta) = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our discrete $0$ and $1$ , we can translate the output of the hypthesis function as follows:\n",
    "$$y = \\begin{cases} 1, & \\text{if $f_\\theta(x) \\geqslant 0.5$} \\\\\n",
    "                    0, & \\text{if $f_\\theta(x) < 0.5$    } \\end{cases}$$\n",
    "\n",
    "We already know that if $z \\geqslant 0$ then our logistic function $g(z) \\geqslant 0.5.$\n",
    "\n",
    "So by assigning $z = \\theta^{T}X,$ we can conclude that $f_\\theta(x) = g(\\theta^{T}X) \\geqslant 0.5$ with $\\theta^{T}x \\geqslant 0$ .\n",
    "\n",
    "To sum up:\n",
    "$$y = \\begin{cases} 1, & \\text{if $\\theta^{T}X \\geqslant 0$} \\\\\n",
    "                    0, & \\text{if $\\theta^{T}X < 0$    } \\end{cases}$$\n",
    "\n",
    "The **decision boundary** is the line that separates the area where $y = 0$ and where $y = 1.$ It is created by our hypothesis function $f_\\theta(x).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "$$\\theta = \\begin{bmatrix} 5 \\\\ -1 \\\\ 0 \\end{bmatrix}~~~~~X = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "$$\\text{So $\\theta^{T}x = 5 + (-1)x_1 + 0x_2$}$$\n",
    "$$\\text{And because $y = 1$ if $\\theta^{T}X \\geqslant 0$}$$\n",
    "$$\\text{Then $5 - x_1 \\geqslant 0$}$$\n",
    "$$\\Rightarrow x_1 \\leqslant 5$$\n",
    "\n",
    "In this case, our decision boundary is a straight vertical line where $x_1 = 5$ , and everything to the left of that denotes $y = 1,$ while everything to the right denotes $y = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One important note:** The input to the sigmoid function $g(z)$ **does not** need to be linear.\n",
    "\n",
    "For example, it could be a function describe a circle $z = \\theta_0 + \\theta_1x_1^{2} + \\theta_2x_2^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. This is why we have to come up with another way to define the **Cost Function**.\n",
    "\n",
    "The **Cost Function** for **Logistic Regression** looks like this:\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{\\text{Cost}(f_\\theta(x^{(i)}),  y^{(i)})}$$\n",
    "With: \n",
    "$$\\text{Cost}(f_\\theta(x^{(i)}),  y^{(i)}) = \\begin{cases} -\\log(f_\\theta(x)) & \\text{if $y = 1$} \\\\\n",
    "                                                           -\\log(1 - f_\\theta(x)) & \\text{if $y = 0$} \\end{cases}$$\n",
    "Here I omitted the superscript $^{(i)}$ to simplify it since it is the same for all training example.\n",
    "\n",
    "From the given definition, we can conclude that:\n",
    "$$\\text{Cost$(f_\\theta(x, y)) = 0$ if $f_\\theta(x)$ = 0}$$\n",
    "$$\\text{Cost$(f_\\theta(x, y)) \\to \\infty$ if $y = 0$ and $f_\\theta(x) \\to 1$}$$\n",
    "$$\\text{Cost$(f_\\theta(x, y)) \\to \\infty$ if $y = 1$ and $f_\\theta(x) \\to 0$}$$\n",
    "\n",
    "The last two equations captures intuition that when the predicted label $f_\\theta(x)$ is wrong (oppose to $y$ ), we will penalize the learning algorithm by a very large cost. \n",
    "\n",
    "**For example**, if a patient with a malignant tumor was predicted as a benign one, the consequence would have been very large.\n",
    "\n",
    "**By writing the Cost Function this way guarantees that $J(\\theta)$ is convex for Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simplifying the Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, this **Cost Function** is too complicated since there are $2$ cases. However, since $y$ has strictly $2$ discrete values: $0$ and $1$ so it can be simplified to this:\n",
    "$$\\text{Cost}(f_\\theta(x, y)) = -y\\log(f_\\theta(x)) - (1 - y)\\log(1 - f_\\theta(x))$$\n",
    "All right, it might look intimidating for you. But if you examine it closely, you will see that:\n",
    "* If $y = 0$ then $\\text{Cost}(f_\\theta(x, y))$ = $-\\log(f_\\theta(x))$\n",
    "* If $y = 1$ then $\\text{Cost}(f_\\theta(x, y))$ = $-\\log(1 - f_\\theta(x))$\n",
    "\n",
    "It's that simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now since we have the simplified $\\text{Cost}(f_\\theta(x, y)).$ We can now plug it back into $J(\\theta):$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{\\text{Cost}(f_\\theta(x^{(i)}),  y^{(i)})}\n",
    "          = -\\frac{1}{m} \\sum_{i=1}^{m}{[y^{(i)}\\log(f_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - f_\\theta(x^{(i)})]}$$\n",
    "The reason behind this **Cost Function** is that it can be derived from statistics using the **The principle of Maximum Likelihood Estimation**, which is an idea in statistics for how to efficiently find parameters' data for different model. And it also has a nice property that it is **Convex**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the parameters $\\theta,$ we have to find the minimized **Cost Function** $\\underset{\\theta}{\\text{min }}J(\\theta).$ After minimized this, we will get the optimized parameters $\\theta,$ which could be put into the hypothesis function $f_\\theta(x)$ to make a prediction given a new $x.$\n",
    "\n",
    "To achieve the optimal $\\theta,$ we will use **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same as "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57fbe0b1856e2b4828360cd3b6d6681c33bcdd458e957abc38b45496cd84ba2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ds': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
