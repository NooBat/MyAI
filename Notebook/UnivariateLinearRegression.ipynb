{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# $$ Univariate~~Linear~~Regresion $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a data set (training set), contains elements which consists of paired values of the independent variable (input) and the dependent variable (output)<br> D = { ( $x_1$ , $y_1$ ), ( $x_2$ , $y_2$ ), … , ( $x_n$ , $y_n$ ) } where $x_i$ is called features and $y_i$ is the **label** on the $i^{th}$ data point.\n",
        "Linear Regression aims to estimate a function fθ with little or no knowledge about the function form.\n",
        "This can be used to predict a label $y$ given a new $x$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **General Form of Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$ f_\\theta (x) = \\theta^{T} \\phi(x) = \\sum_{i=1}^{m}{\\theta_i \\phi_i (x)} $$ \n",
        "* $f_\\theta (x)$ is called the hypothesis function.\n",
        "* $\\phi(x)$ is a basis function (by analogy with the concept of vectors are composed of a linear combination of basis vectors).\n",
        "* $\\theta$ is the coefficient of the linear combination. In neural network it is often referred as '**Weight**'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cost Function (Loss Function), sometimes called Sum of Squared estimate of Errors (SSE).\n",
        "This is calculated by summing up all the squared differences between predicted and actual label $y$ .\n",
        "This is the important criteria to evaluate a Linear Regression model.\n",
        "The general formula for calculating the Cost Function is:\n",
        "$$ J(θ_0 , θ_1 , … , θ_n ) =  \\frac{1}{2m} \\sum_{i=1}^{m}{( y_i - f_θ(x_i) )^2} $$\n",
        "* $J(θ_0 , θ_1 , … , θ_n )$ is the Cost Function.\n",
        "* $f_θ(x)$ is the hypothesis.\n",
        "* $m$ is the size of the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Usage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can be used to calculate how fit the hypothesis is with the given set of data. The hypothesis will be the fittest when Cost Function is at its extreme minimum. This can be achieved by using Gradient Descent algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A first-order iterative optimization algorithm for finding a local minimum of a differentiable function. This idea is to take repeated steps in the opposite direction of the gradient of the cost function $J(θ_0 , θ_1 , … , θ_n)$ at the current point. We will use the $\\theta$ as a matrix vector containing all $\\theta$.\n",
        "$$\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ ... \\\\ \\theta_n \\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repeat until converges:<br>\n",
        "$$\\theta_{i+1} = \\theta_{i} - \\alpha \\nabla J(\\theta) $$\n",
        "\n",
        "With<br> \n",
        "$$\\nabla J(\\theta) = \\begin{bmatrix} \\frac{\\partial J(\\theta_0)}{\\partial \\theta_0} \\\\\\\\ \\frac{\\partial J(\\theta_1)}{\\partial \\theta_1} \\\\\\\\ ... \\\\\\\\ \\frac{\\partial J(\\theta_n)}{\\partial \\theta_n} \\end{bmatrix} $$\n",
        "\n",
        "* $\\alpha$ is the learning rate. \n",
        "* $\\nabla J(\\theta)$ is the gradient of the cost function $J(\\theta)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZUkreM_0owa"
      },
      "source": [
        "# Read input, build DataFrame and plot the data points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "vowkjTxeK5Yz",
        "outputId": "7214d637-3b4e-481b-8cc5-2860454bc28d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(100)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
        "\n",
        "df = pd.read_csv('/Users/danielnguyen/Repo/AI/Training_set/likes.csv')\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "I14JmCqOK1aG",
        "outputId": "6c2261ac-d29b-4aa3-d677-b81c81c43670"
      },
      "outputs": [],
      "source": [
        "data0 = np.array(df.loc[:, 'Day':'Likes'])\n",
        "\n",
        "#Shuffle the data\n",
        "data1 = np.random.permutation(data0)\n",
        "\n",
        "#Sort the first 70 pairs (x, y) and use it as our training set\n",
        "training_set = np.sort(data1[:70].T).T\n",
        "\n",
        "#Shows the first 10 data\n",
        "print(training_set[:10].T)\n",
        "\n",
        "#Arrays of x and y\n",
        "days = np.sort(training_set[:70, 0])\n",
        "likes = np.sort(training_set[:70, 1])\n",
        "\n",
        "#Training set size\n",
        "m = len(likes)\n",
        "\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Likes')\n",
        "plt.scatter(days, likes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F37nP4RI0-VN"
      },
      "source": [
        "# Choosing the hypothesis function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9tglbATu56K"
      },
      "source": [
        "Here is the plot for the total number of likes of the **GDSC**'s Facebook page for 107 days from **31/12/2020**. First, we choose the hypothesis fucntion $f$<sub>$θ$</sub>($x$) = $θ$<sub>0</sub> + $θ$<sub>1</sub>$x$ (linear one-variable function). We will use θ<sub>0</sub> = 2186 and θ<sub>1</sub> = 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-fzFQvGzYW6",
        "outputId": "5fbce95a-fb3d-4e23-d0f2-c5415000bcf0"
      },
      "outputs": [],
      "source": [
        "theta0 = 2186\n",
        "theta1 = 0\n",
        "\n",
        "t_vect = np.array([[theta0], [theta1]])\n",
        "print(t_vect)\n",
        "\n",
        "#Change the x matrix to a (m x 2) matrix\n",
        "\n",
        "x_0 = np.ones((m, 1))\n",
        "x_1 = days.reshape(m, 1)\n",
        "\n",
        "x = np.hstack((x_0, x_1))\n",
        "y = likes.reshape(m, 1)\n",
        "\n",
        "mean = np.mean(y)\n",
        "\n",
        "print(x[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3lQz7KW1P14"
      },
      "source": [
        "# Find the Cost Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88BxAuc23bqv"
      },
      "source": [
        "Now we write the cost function for this hypothesis.\n",
        "$$ J(\\theta_0 ,θ_1) = \\frac{1}{2m} \\sum_{i=1}^{m} {(f_θ(x_i) - y_i)^2} $$ \n",
        "With $m$ is the number of training\n",
        "example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0a3X9OwfChn"
      },
      "outputs": [],
      "source": [
        "def calculate_cost(t_vect, x_matrix, y_matrix) : \n",
        "    \"\"\"Calculate the cost function for linear regression model\"\"\"\n",
        "\n",
        "    cost_matrix = np.dot(x_matrix, t_vect) - y_matrix\n",
        "    \n",
        "    return np.sum(np.square(cost_matrix)) / (2 * m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "Ih7Uod-bFyav",
        "outputId": "d3aa7c47-6506-4fbb-94e5-67ae25af7866"
      },
      "outputs": [],
      "source": [
        "theta1_lst = np.linspace(-4, 8)\n",
        "cost_lst = []\n",
        "\n",
        "#Calculate the cost function for every theta1 from -4 to 8\n",
        "for theta in theta1_lst:\n",
        "    t_vect_temp = np.array([[theta0], [theta]])\n",
        "    cost_lst.append(calculate_cost(t_vect_temp, x, y))\n",
        "cost_lst = np.array(cost_lst)\n",
        "\n",
        "plt.xlabel(r'$\\theta_1$')\n",
        "plt.ylabel('J(' + r'$\\theta_1)$')\n",
        "\n",
        "plt.scatter(theta1, calculate_cost(t_vect, x, y), c='red')\n",
        "plt.plot(theta1_lst, cost_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8JySBwt1V8s"
      },
      "source": [
        "# Use Gradient Descent to optimize the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDXeXn3rkj7-"
      },
      "source": [
        "From the plot of data above, we can see that the line has not fit the data, which means the hypothesis $h$<sub>$θ$</sub> is not sufficient for predicting new values. This is called UNDERFITTING. If we want to be more precisely, we have to minimize the result of cost function $J$($θ$<sub>0</sub>, $θ$<sub>1</sub>), meaning find the local minima of the function. To achieve this, we use the Gradient Descent. The GD formula for this will be:<br><br>\n",
        "\n",
        "$~~~~θ$<sub>0</sub> = $θ$<sub>1</sub> - $α$ $\\frac{∂}{∂θ_0}$ $J$($θ$<sub>0</sub>)<br><br> \n",
        "$~~~~θ$<sub>1</sub> = $θ$<sub>1</sub> - $α$ $\\frac{∂}{∂θ_1}$ $J$($θ$<sub>1</sub>)<br><br>\n",
        "\n",
        "This is equivalent to:<br><br> \n",
        "$~~~~θ$<sub>0</sub> = $θ$<sub>0</sub> - $α$ $\\frac{1}{m}$ $\\sum_{i=1}^m$ (($f$<sub>$θ$</sub>($x$<sub>i</sub>) - $y$<sub>i</sub>))<br><br> \n",
        "$~~~~θ$<sub>1</sub> = $θ$<sub>1</sub> - $α$ $\\frac{1}{m}$ $\\sum_{i=1}^m$ (($f$<sub>$θ$</sub>($x$<sub>i</sub>) - $y$<sub>i</sub>)$x$<sub>i</sub>)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi3F9WAHpQI-"
      },
      "outputs": [],
      "source": [
        "def gradient_func(theta, x_matrix, y_matrix):\n",
        "    \"\"\"Find the gradient of the given model\"\"\"\n",
        "\n",
        "    gradient_matrix = np.dot(x_matrix.T, (np.subtract(np.dot(x_matrix, theta), y_matrix)))\n",
        "    \n",
        "    return gradient_matrix / m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgiZ2dM9GBBR"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(theta, x_matrix, y_matrix, threshhold = 1.0e-6, max_iterations=1000, learning_rate=1e-4) :\n",
        "    \"\"\"Optimize θ1 to minimize the cost function and return 2 arrays contain points \n",
        "    on the cost function plot for which the gradient descent visited each iteration\"\"\"\n",
        "\n",
        "    theta1_history = theta[1][0]\n",
        "    j_history = calculate_cost(theta, x_matrix, y_matrix)\n",
        "    i = 0\n",
        "    diff = 1e10\n",
        "\n",
        "    while i < max_iterations and diff > threshhold:\n",
        "        theta = theta - learning_rate * gradient_func(theta, x_matrix, y_matrix)\n",
        "        theta1_history = np.vstack((theta1_history, theta[1][0]))\n",
        "        j_history = np.vstack((j_history, calculate_cost(theta, x_matrix, y_matrix)))\n",
        "\n",
        "        i += 1\n",
        "        diff = np.absolute(j_history[-1] - j_history[-2])\n",
        "        \n",
        "    return theta, theta1_history, j_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "weLOTGh57pao",
        "outputId": "524d73fc-6183-4b53-af6f-e97b8f30a921"
      },
      "outputs": [],
      "source": [
        "t_vect, theta1_arr, j_arr = gradient_descent(t_vect, x, y)\n",
        "\n",
        "print(t_vect)\n",
        "\n",
        "plt.scatter(theta1_arr.T, j_arr.T, c='red', marker='+')\n",
        "plt.plot(theta1_lst, cost_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "y_Kh_IFEI-AF",
        "outputId": "b0e5a242-9cce-42b2-ee1a-507d1618dea4"
      },
      "outputs": [],
      "source": [
        "plt.scatter(days, likes)\n",
        "plt.plot(days, np.dot(x, t_vect).reshape(1, m)[0], color='red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpPjIbMAKbsM"
      },
      "source": [
        "# Gradient Descent Performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This step is to check the performance of GD. For example, by looking at this plot, we can know if we had chosen the correct learning rate $ \\alpha $ or not. The plot gets steeper if we chose the correct learning rate. If it just a horizontal line, that means your learning rate is too large, making the algorithm diverge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "bP-FtdHxKeiK",
        "outputId": "2699f10f-e58c-49f7-a6cc-2b741a15d8ab"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1, len(j_arr) + 1), j_arr.reshape(1, len(j_arr))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP0FoZPM1fLh"
      },
      "source": [
        "# Testing and evaluating the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nvk8c5O1mGC"
      },
      "source": [
        "After finish optimizing the model, we use the unused data to test our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6zKbsMfJJrg"
      },
      "outputs": [],
      "source": [
        "#test \n",
        "def get_num_likes(day=0):\n",
        "    global t_vect\n",
        "    return t_vect[0][0] + t_vect[1][0] * day\n",
        "\n",
        "col_names = ['Day','Predicted','Actual','Error'] \n",
        "test_set = np.sort(data1[70:].T).T \n",
        "predicted_values = np.around(get_num_likes(test_set[0:, 0]))\n",
        "real_values = test_set[0:, 1]\n",
        "\n",
        "diff = predicted_values - real_values\n",
        "\n",
        "with open('/Users/danielnguyen/Repo/AI/Testing_set/test.csv', 'w') as f:\n",
        "    f.write(','.join(col_names) + '\\n')\n",
        "    for i in range(len(test_set)):\n",
        "        f.write(','.join([str(test_set[i][0]), str(predicted_values[i]), \n",
        "                          str(real_values[i]), str(diff[i] / real_values[i] * 100)]) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5NRdr-DMOUm"
      },
      "source": [
        "To how fit is the model, we calculate $ R^2 $ (R-Squared value) of the model.\n",
        "<br>\n",
        "$R^2$ = 1 - $\\frac{J}{SSTO}$<br><br>\n",
        "\n",
        "SSTO (Total Sum of Square) is calculated according to this formula:<br>\n",
        "$SSTO$ = $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(y_{mean} - y_i)^2$<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPUEGKmnMOHm",
        "outputId": "b7da4854-9d6c-4888-fe62-aa05a9218ac9"
      },
      "outputs": [],
      "source": [
        "SSTO = np.sum(np.square(mean - y)) / (2 * m)\n",
        "\n",
        "r_square = 1 - (calculate_cost(t_vect, x, y) / SSTO)\n",
        "\n",
        "print('R-Squared Value = ' + str(r_square))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djqxi4IPkpN"
      },
      "source": [
        "The $R^2$ is 0.92, which is a good fit model, can make good predictions without being overfitted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEVODKyHo_ju"
      },
      "source": [
        "# Changing the hypothesis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTtYcY_qpsNw"
      },
      "source": [
        "In the previous model, the $R^2$ = 0.92, but we can do better. Now I am going to change my hypothesis by using a different basis function $h$<sub>$θ$</sub>($x$) = $θ$<sub>0</sub> + $θ$<sub>1</sub> $\\sqrt{x} $ + $θ$ <sub>2</sub>$x$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "6TYovQ-5ESis",
        "outputId": "264e37ca-60bb-476f-804a-8afff32ceb86"
      },
      "outputs": [],
      "source": [
        "t0 = 2199.55779\n",
        "t1 = 8.622317\n",
        "t2 = 1.998494\n",
        "\n",
        "plt.plot(x, t0 + t1 * x ** 0.5 + t2 * x, c='red')\n",
        "plt.scatter(days, likes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we also try a polynomial function with a degree equals 3. $g$<sub>$θ$</sub>($x$) = $θ$<sub>0</sub> + $θ$<sub>1</sub> $x$ + $θ$<sub>2</sub> $x^2$ + $\\theta$<sub>3</sub> $x^3$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta0 = 2202.2975324712297\n",
        "theta1 = 5.01399360\n",
        "theta2 = -5.49625136e-2\n",
        "theta3 = 3.43653139e-4\n",
        "\n",
        "plt.scatter(days, likes)\n",
        "plt.plot(days, theta0 + theta1 * days + theta2 * days ** 2 + theta3 * days ** 3, c='green')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mze9al2XasS"
      },
      "outputs": [],
      "source": [
        "def hypothesis_3(theta0, theta1, theta2, theta3, x_matrix):\n",
        "    return (theta0 + theta1 * x_matrix + theta2 * x_matrix ** 2 + theta3 * x_matrix ** 3)\n",
        "def cost_func2(theta0, theta1, theta2, theta3, x_matrix, y_matrix) :\n",
        "    return np.sum((hypothesis_3(theta0, theta1, theta2, theta3, x_matrix) - y_matrix) ** 2) / (2 * len(x_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2ldO_1dEvmJ",
        "outputId": "019aed07-203d-4be1-e7df-cd6016b96d3a"
      },
      "outputs": [],
      "source": [
        "#test \n",
        "def get_num_likes(day=0):\n",
        "    global t0, t1, t2\n",
        "    return t0 + t1 * day ** 0.5 + t2 * day\n",
        "\n",
        "def cost_func1(theta0, theta1, theta2, x_matrix, y_matrix) :\n",
        "    return np.sum((theta0 + theta1 * x_matrix ** 0.5 + theta2 * x_matrix - y_matrix) ** 2) / (2 * len(x_matrix))\n",
        "\n",
        "r_square = 1 - cost_func1(t0, t1, t2, days, likes) / SSTO\n",
        "print(\"R-Squared value for h(x) \" + str(r_square))\n",
        "\n",
        "r_square = 1 - cost_func2(theta0, theta1, theta2, theta3, days, likes) / SSTO\n",
        "print(\"R-Squared value for g(x) = \" + str(r_square))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkgg5TVtQ2Ao"
      },
      "source": [
        "The $R^2$ = 0.97895 and 0.98745 which is higher than before. Hence, base on the demand, we can choose a simple or a complicated hypothesis (unless it is underfitted or overfitted). Although higher $R^2$ can sometimes lead to overfitting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LinearRegressionDemo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
