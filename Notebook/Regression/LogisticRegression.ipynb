{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Logistic~~Regression$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this is called Regression, it falls under another category of **Supervised Learning** which is **Classification**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification** is a process of categorizing a given set of data into **classes**. It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points. The classes are often referred to as **targets**, **labels** or **categories**.\n",
    "\n",
    "The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which **class**/**category** the new data will fall into.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Given the size of the cancer tumour, we can predict whether this is a benign or malignant tumour.\n",
    "- Given the annual income of an individual, banks can predict whether that individual is capable of paying the debt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the label that we want to predict take on a small number of discrete values. For **Binary Classification**, the label $y$ only take on two values: $0$ (malignant, cannot pay debt) and $1$ (benign, can pay debt) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to use linear regression and map all predictions greater than $0.5$ as $1$ and all less than $0.5$ as $0$ . But this doesn't work well because classification is not actually a linear function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multiclass Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiclass Classification** occurs when the data set has more than two categories. So instead of $y \\in \\{0, 1\\}$ we will expand it so that $y \\in \\{0, 1, 2, ..., n\\}.$\n",
    "\n",
    "Since we have $n + 1$ discrete categories, we can divide our problems into $n + 1$ **Binary Classification** problems; in each one, we predict the corresponding $y.$ This approach is called **One-vs-All**.\n",
    "\n",
    "$$y \\in \\{0, 1, 2, ..., n\\}$$\n",
    "We perform **Logistic regression** for each category $y$ to get:\n",
    "$$f_\\theta^{(0)} = P(y=0|x;\\theta)$$\n",
    "$$f_\\theta^{(1)} = P(y=1|x;\\theta)$$\n",
    "$$f_\\theta^{(2)} = P(y=2|x;\\theta)$$\n",
    "$$...$$\n",
    "$$f_\\theta^{(n)} = P(y=n|x;\\theta)$$\n",
    "And then the prediction will be the best probability in all categories $\\underset{i}{\\text{max}}(f_\\theta^{(i)}(x))$\n",
    "\n",
    "Simply speaking, we are choosing one class and lumping all the others into a single second class. By repeating this process and apply binary logistic regression to each cases, and then use the hypothesis function that returned the highest value as our prediction (since with the given value $x,$ the higher the odd, the more likely it is correct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General formula**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want our hypothesis function to give certain discrete values ( $0 \\leqslant f_\\theta(x) \\leqslant 1 $ ), which cannot be achieved with normal linear function $f_\\theta(x) = \\theta^{T} x$ . That is why we are using something called the **Sigmoid function**:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "$$\\Rightarrow f_\\theta(x) = g(\\theta_{T}x) = \\frac{1}{1 + e^{-\\theta_{T}x}}$$\n",
    "This is the reason why **Logistic function** can be used interchangeably with **Sigmoid function**.\n",
    "\n",
    "Here is how the Sigmoid function look like:\n",
    "\n",
    "![Sigmoid function illustration](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/1WFqZHntEead-BJkoDOYOw_2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png?expiry=1638662400000&hmac=8EA0eH0xwVK5nkS2SMV-L8FDqmb3Sc_vHJ-PG5r-gz0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Usage**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to predict the probability that the label $y = 1$ on input $x$ .\n",
    "For example, if:\n",
    "$$x = \\begin{bmatrix} x_0 \\\\ x_1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\text{Tumor Size} \\end{bmatrix} $$\n",
    "$$\\Rightarrow f_\\theta(x) = 0.7 $$\n",
    "Then this tells that there is a 70% chance that the patient's tumor is malignant.\n",
    "\n",
    "Statistically, this could be written as: \n",
    "$$f_\\theta(x) = P(y = 1 | x; \\theta)~~\\text{Meaning: Probability that $y = 1$, given $x$, parameterized by $\\theta$}$$\n",
    "Since there are only two possible outcomes then\n",
    "$$P(y = 0 | x; \\theta) + P(y = 1 | x; \\theta) = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our discrete $0$ and $1$ , we can translate the output of the hypthesis function as follows:\n",
    "$$y = \\begin{cases} 1, & \\text{if $f_\\theta(x) \\geqslant 0.5$} \\\\\n",
    "                    0, & \\text{if $f_\\theta(x) < 0.5$    } \\end{cases}$$\n",
    "\n",
    "We already know that if $z \\geqslant 0$ then our logistic function $g(z) \\geqslant 0.5.$\n",
    "\n",
    "So by assigning $z = \\theta^{T}X,$ we can conclude that $f_\\theta(x) = g(\\theta^{T}X) \\geqslant 0.5$ with $\\theta^{T}x \\geqslant 0$ .\n",
    "\n",
    "To sum up:\n",
    "$$y = \\begin{cases} 1, & \\text{if $\\theta^{T}X \\geqslant 0$} \\\\\n",
    "                    0, & \\text{if $\\theta^{T}X < 0$    } \\end{cases}$$\n",
    "\n",
    "The **decision boundary** is the line that separates the area where $y = 0$ and where $y = 1.$ It is created by our hypothesis function $f_\\theta(x).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "$$\\theta = \\begin{bmatrix} 5 \\\\ -1 \\\\ 0 \\end{bmatrix}~~~~~X = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "$$\\text{So $\\theta^{T}x = 5 + (-1)x_1 + 0x_2$}$$\n",
    "$$\\text{And because $y = 1$ if $\\theta^{T}X \\geqslant 0$}$$\n",
    "$$\\text{Then $5 - x_1 \\geqslant 0$}$$\n",
    "$$\\Rightarrow x_1 \\leqslant 5$$\n",
    "\n",
    "In this case, our decision boundary is a straight vertical line where $x_1 = 5$ , and everything to the left of that denotes $y = 1,$ while everything to the right denotes $y = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One important note:** The input to the sigmoid function $g(z)$ **does not** need to be linear. You can use polynomial features to fit the model better.\n",
    "\n",
    "For example, it could be a function describe a circle $z = \\theta_0 + \\theta_1x_1^{2} + \\theta_2x_2^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the same cost function that we use for **Linear Regression** because the **Logistic Function** will cause the graph of the **Cost Function** to be wavy, resulting in many local optimas. In other words, it will not be a convex function. This is why we have to come up with another way to define the **Cost Function**.\n",
    "\n",
    "The **Cost Function** for **Logistic Regression** looks like this:\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{\\text{Cost}(f_\\theta(x^{(i)}),  y^{(i)})}$$\n",
    "With: \n",
    "$$\\text{Cost}(f_\\theta(x^{(i)}),  y^{(i)}) = \\begin{cases} -\\log(f_\\theta(x)) & \\text{if $y = 1$} \\\\\n",
    "                                                           -\\log(1 - f_\\theta(x)) & \\text{if $y = 0$} \\end{cases}$$\n",
    "Here I omitted the superscript $^{(i)}$ to simplify it since it is the same for all training example.\n",
    "\n",
    "From the given definition, we can conclude that:\n",
    "$$\\text{Cost$(f_\\theta(x, y)) = 0$ if $f_\\theta(x)$ = 0}$$\n",
    "$$\\text{Cost$(f_\\theta(x, y)) \\to \\infty$ if $y = 0$ and $f_\\theta(x) \\to 1$}$$\n",
    "$$\\text{Cost$(f_\\theta(x, y)) \\to \\infty$ if $y = 1$ and $f_\\theta(x) \\to 0$}$$\n",
    "\n",
    "The last two equations captures intuition that when the predicted label $f_\\theta(x)$ is wrong (oppose to $y$ ), we will penalize the learning algorithm by a very large cost. \n",
    "\n",
    "**For example**, if a patient with a malignant tumor was predicted as a benign one, the consequence would have been very large.\n",
    "\n",
    "**By writing the Cost Function this way guarantees that $J(\\theta)$ is convex for Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simplifying the Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, this **Cost Function** is too complicated since there are $2$ cases. However, since $y$ has strictly $2$ discrete values: $0$ and $1$ so it can be simplified to this:\n",
    "$$\\text{Cost}(f_\\theta(x, y)) = -y\\log(f_\\theta(x)) - (1 - y)\\log(1 - f_\\theta(x))$$\n",
    "All right, it might look intimidating for you. But if you examine it closely, you will see that:\n",
    "* If $y = 0$ then $\\text{Cost}(f_\\theta(x, y))$ = $-\\log(f_\\theta(x))$\n",
    "* If $y = 1$ then $\\text{Cost}(f_\\theta(x, y))$ = $-\\log(1 - f_\\theta(x))$\n",
    "\n",
    "It's that simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now since we have the simplified $\\text{Cost}(f_\\theta(x, y)).$ We can now plug it back into $J(\\theta):$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{\\text{Cost}(f_\\theta(x^{(i)}),  y^{(i)})}\n",
    "          = -\\frac{1}{m} \\sum_{i=1}^{m}{\\left[y^{(i)}\\log(f_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - f_\\theta(x^{(i)})\\right]}$$\n",
    "This function has another name: **Binary Cross-Entropy Loss**.\n",
    "\n",
    "The reason behind this **Cost Function** is that it can be derived from statistics using the **the Principle of Maximum Likelihood Estimation**, which is an idea in statistics for how to efficiently find parameters' data for different model. And it also has a nice property that it is **Convex**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the parameters $\\theta,$ we have to find the minimized **Cost Function** $\\underset{\\theta}{\\text{min }}J(\\theta).$ After minimized this, we will get the optimized parameters $\\theta,$ which could be put into the hypothesis function $f_\\theta(x)$ to make a prediction given a new $x.$\n",
    "\n",
    "To achieve the optimal $\\theta,$ we will use some optimization algorithm. But for simplicity, we will use **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vectorized Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h = g(\\theta X)$$\n",
    "$$J = \\frac{1}{m} \\cdot \\left( -y^{T}\\log(h) - (1 - y)^{T}\\log(1 - h) \\right) $$\n",
    "\n",
    "With: \n",
    "* $\\theta$ is a vector with a dimension of $n \\times 1$ contains $\\theta_0, \\theta_1, ..., \\theta_n.$\n",
    "* $X$ is a matrix with a dimension of $m \\times n$ contains every training examples on each row.\n",
    "* $y$ is a vector with a dimension of $n \\times 1$ contains every training label on each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is still the same:\n",
    "\n",
    "$\\text{repeat until convergence:}$ $$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)~~\\text{(simutaneously update all $\\theta_j$)}$$\n",
    "\n",
    "Since $$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{[y^{(i)}\\log(f_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - f_\\theta(x^{(i)})]}$$\n",
    "\n",
    "In which $f_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n)$\n",
    "\n",
    "The derivative now will be:\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{\\left[(f_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\right]}$$\n",
    "\n",
    "I won't prove since it is very long and confusing, but if you understand Calculus then you could prove it yourself.\n",
    "\n",
    "So the **Gradient Descent** algorithm will become:\n",
    "\n",
    "$\\text{repeat until convergence:}$ $$\\theta_j = \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m}{\\left[(f_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\right]}~~\\text{(simutaneously update all $\\theta_j$)}$$\n",
    "\n",
    "If you take a closer look, you will see that the algorithm looks identical to **Linear Regression**!!! But it is not the same since the hypothesis in **Logistic Regression** is different from **Linear Regression**.\n",
    "\n",
    "So even though the $\\theta~-$ updating rule looks identical, it is not the same as the Gradient Descent for **Linear Regression** because the definition of the hypothesis function has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vectorized Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = \\theta - \\frac{\\alpha}{m} X^{T} \\left( g(X\\theta) - y \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regularized Gradient Descent for Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle is the same as before: add a regularizing term to the original **Cost Function** to penalize $\\theta$ parameters, and of course, we have to exclude the bias term $\\theta_0.$\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{\\left[ y^{(i)}\\log(f_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - f_\\theta(x^{(i)})) \\right]} + \\frac{\\lambda}{2m} \\sum_{j=1}^{n}{\\theta_j^2}$$\n",
    "\n",
    "By taking partial derivative for each $\\theta_j$ and update $\\theta_0$ separately we have the regularized algorithm:\n",
    "\n",
    "$\\text{Repeat until convergence:}$\n",
    "$$\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{n}{\\left[ (f_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\right]}$$\n",
    "$$\\theta_j = \\theta_j - \\alpha \\cdot \\left\\{ \\frac{1}{m} \\sum_{i=1}^{n}{\\left[ (f_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\right] +\\frac{\\lambda}{m} \\theta_j} \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from **Gradient Descent**, we have other more advanced algorithms such as: **Conjugate Gradient**, **BFGS**, **L-BFGS**. One of their advantages is that it does not need to manually choose the learning rate $\\alpha.$ But these are very complex algorithms to implement. So we should use predefined function from libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read input, build DataFrame and plot data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Years of Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Loan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.3</td>\n",
       "      <td>10702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2</td>\n",
       "      <td>11180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.1</td>\n",
       "      <td>11566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.3</td>\n",
       "      <td>12669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1</td>\n",
       "      <td>13055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Years of Experience  Salary  Loan\n",
       "0                  6.3   10702     0\n",
       "1                  4.2   11180     0\n",
       "2                  6.1   11566     0\n",
       "3                  3.3   12669     0\n",
       "4                  5.1   13055     0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd         \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
    "\n",
    "header_names = ['Years of Experience', 'Salary', 'Loan']\n",
    "\n",
    "df = pd.read_csv('../../Training_set/loan_salary_data.csv', names=header_names, header=0)\n",
    "\n",
    "m = len(df)\n",
    "n = len(df.columns)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement $\\theta$ as a $3 \\times 1$ dimensions matrix, $X$ as a $m \\times 3$ dimensions matrix by adding a bias term with all $1$ and $Y$ as a $m \\times 1$ dimensions matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[1.0000e+00 6.3000e+00 1.0702e+04]\n",
      " [1.0000e+00 4.2000e+00 1.1180e+04]\n",
      " [1.0000e+00 6.1000e+00 1.1566e+04]\n",
      " [1.0000e+00 3.3000e+00 1.2669e+04]\n",
      " [1.0000e+00 5.1000e+00 1.3055e+04]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array(df)\n",
    "\n",
    "X = np.array(df.iloc[:, 0:2])\n",
    "X = np.hstack((np.ones((m, 1)), X))\n",
    "y = np.array(df.iloc[:, [2]])\n",
    "\n",
    "theta = np.zeros((3, 1))\n",
    "\n",
    "print(theta)\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the data points onto a plot to have a better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y): \n",
    "    positive = np.where(y == 1)\n",
    "    negative = np.where(y == 0)\n",
    "\n",
    "    plt.scatter(X[positive, -2], X[positive, -1], s=60, c='green', marker='+', )\n",
    "    plt.scatter(X[negative, -2], X[negative, -1], s=60, c='yellow', marker='o', edgecolors='black')\n",
    "    plt.xlabel('Years of Experience')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.legend(['Can pay loan', 'Cannot pay loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypthesis, Sigmoid Function and Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the hypothesis is defined as: $$f_\\theta(x) = g(\\theta^{T}X)$$\n",
    "where function $g$ is the Sigmoid Function. The Sigmoid Function is defined as:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "So first we implement the Sigmoid Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) :\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function and Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Cost Function and Gradient for the Logistic Regression. Recall that the cost function in logistic function is:\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{\\left[ y^{(i)}\\log(f_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - f_\\theta(x^{(i)})) \\right]},$$\n",
    "and the Gradient of the cost is a vector of the same length as $\\theta$ where $j^{th}$ (for $j \\in [0, 1, ..., n]$) is defined as:\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{\\left( f_\\theta(x^{(i)}) - y^{(i)} \\right)x^{(i)}}$$\n",
    "\n",
    "So we will define a function that return the result of the Cost Function and the matrix contains the Gradient of each $\\theta.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y) :\n",
    "    m = len(y)\n",
    "    z = np.dot(X, theta)\n",
    "\n",
    "    J = np.dot(y.T, np.log(sigmoid(z)) + np.dot((1 - y).T, np.log(1 - sigmoid(z)))) / -m\n",
    "\n",
    "    grad = np.dot(X.T, sigmoid(z) - y) / m\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at the initial theta (zeros): 14.15753116293688\n"
     ]
    }
   ],
   "source": [
    "cost, grad = costFunction(theta, X, y)\n",
    "print('Cost at the initial theta (zeros):', cost[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the initial theta: [[-3.7500000e-02]\n",
      " [-5.6375000e-01]\n",
      " [-1.0819175e+04]]\n"
     ]
    }
   ],
   "source": [
    "print('Gradient of the initial theta:', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Descent for Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try and use our data to plot a Decision Boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(theta, X, y) :\n",
    "    plotData(X, y)\n",
    "\n",
    "    plotX = np.array(X[:, -2].min(), X[:, -2].max())\n",
    "    plotY = \n",
    "\n",
    "    plt.plot(plotX, plotY, c='red', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDecisionBoundary(theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(theta, X, y, max_iterations=500, learning_rate=0.001, threshold=1e-3):\n",
    "    J_history, grad = costFunction(theta, X, y)\n",
    "\n",
    "    diff = 1e10\n",
    "    i = 0\n",
    "\n",
    "    while i < max_iterations and diff > threshold:\n",
    "        theta = theta - learning_rate * grad\n",
    "\n",
    "        J, grad = costFunction(theta, X, y)\n",
    "\n",
    "        J_history = np.hstack(J_history, J)\n",
    "\n",
    "        diff = J_history[-2] - J_history[-1]\n",
    "        i += 1\n",
    "\n",
    "    return theta, J_history"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57fbe0b1856e2b4828360cd3b6d6681c33bcdd458e957abc38b45496cd84ba2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ds': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
