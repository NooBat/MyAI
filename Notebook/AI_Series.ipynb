{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Import libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94oYiPqBa6du"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.path.insert(0, '../')\n",
        "\n",
        "from generate_version import generate_version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Prepare the dataset for loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_label_dir(df, dir='../gdsc-ai-challenge/train'):\n",
        "    \"\"\"Use Dataframe contains labels for each image and path to the directory\n",
        "\n",
        "    contains the unlabeled dataset to rebuild directory into labeled subdirectories.\n",
        "\n",
        "    Returns all the label and number of classes in the dataset.\n",
        "\n",
        "    Keyword arguments:\n",
        "\n",
        "    df -- The Dataframe contains images' names and labels.\n",
        "\n",
        "    dir -- Path to the main directory (default to ../gdsc-ai-challenge/train)\n",
        "    \"\"\"\n",
        "    class_names = np.sort(df['label'].unique())\n",
        "    number_of_classes = len(class_names)\n",
        "\n",
        "    if not os.path.exists(dir):\n",
        "        return class_names, number_of_classes\n",
        "\n",
        "    for class_name in class_names:\n",
        "        subdir = pathlib.Path(os.path.join(dir, class_name))\n",
        "        if subdir.exists():\n",
        "            continue\n",
        "        else:\n",
        "            subdir.mkdir()\n",
        "    \n",
        "    return class_names, number_of_classes\n",
        "\n",
        "def sort_data(df, dir='../gdsc-ai-challenge/train'):\n",
        "    \"\"\"Use Dataframe to move each unlabeled image to the correct label's subdirectory.\n",
        "\n",
        "    df -- The Dataframe contains images' names and labels.\n",
        "\n",
        "    dir -- Path to the main directory (default to ../gdsc-ai-challenge/train) \n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir):\n",
        "        return\n",
        "    \n",
        "    unlabeled_dir = os.path.join(dir, 'train')\n",
        "\n",
        "    for image_dir in [str(img) for img in list(pathlib.Path(unlabeled_dir).glob('*.png'))]:\n",
        "        id = int(image_dir.removeprefix(unlabeled_dir).removesuffix('.png'))\n",
        "        label = df['label'][id - 1]\n",
        "        dest_path = os.path.join(dir, label, str(id) + '.png')\n",
        "        shutil.move(image_dir, dest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_df = pd.read_csv('../gdsc-ai-challenge/trainLabels.csv')\n",
        "\n",
        "class_names, number_of_classes = create_label_dir(label_df)\n",
        "sort_data(label_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data preprocessing and augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    \"\"\"Split the dataset into three subsets: train, validation (dev) and test set.\n",
        "\n",
        "    Returns three tuples, containing each subset with its size.\n",
        "\n",
        "    Keyword arguments:\n",
        "\n",
        "    ds -- tf.data.Dataset object\n",
        "\n",
        "    ds_size -- size of the dataset\n",
        "\n",
        "    train_split -- percentage to split into train set (default to 0.8)\n",
        "\n",
        "    val_split -- percentage to split into validation set (default to 0.1)\n",
        "\n",
        "    test_split -- percentage to split into test set (default to 0.1)\n",
        "    \"\"\"\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return (train_ds, train_size), (val_ds, val_size), (test_ds, ds_size - val_size - train_size)\n",
        "\n",
        "def configure(ds, ds_size, batch_size=32, shuffle=True, augment=False):\n",
        "    \"\"\"Configure the given dataset for better performance (by caching, prefetching and then batching the dataset)\n",
        "\n",
        "    and perform preprocessing to the images in the given dataset.\n",
        "\n",
        "    Returns the optimized dataset.\n",
        "\n",
        "    Keyword arguments:\n",
        "\n",
        "    ds -- tf.data.Dataset object\n",
        "\n",
        "    ds_size -- size of the dataset\n",
        "\n",
        "    batch_size -- size of each batch (default to 32)\n",
        "\n",
        "    shuffle -- whether to shuffle the dataset (default to True)\n",
        "\n",
        "    augment -- whether to perform data augmentation to the dataset (default to False)\n",
        "    \"\"\"\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    rescale = keras.layers.Rescaling(1.0/255)\n",
        "    data_augmentation = keras.Sequential([\n",
        "        keras.layers.RandomFlip('horizontal'),\n",
        "        keras.layers.RandomRotation(0.05, fill_mode='nearest')\n",
        "    ])\n",
        "\n",
        "    ds = ds.map(lambda x, y: (rescale(x), y),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    ds = ds.cache()\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=int(ds_size * 0.6))\n",
        "    \n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    if augment:\n",
        "        with tf.device('/cpu:0'):\n",
        "            #only perform data augmentation on train set\n",
        "            ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
        "                                    num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset using *image_dataset_from_directory()*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    '../gdsc-ai-challenge/train',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=None,\n",
        "    image_size=(32,32),\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "ds_size = ds.cardinality().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the dataset and Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(train_ds, train_size), (val_ds, val_size), (test_ds, test_size) = split_dataset(ds, ds_size, train_split=0.7, val_split=0.2, test_split=0.1)\n",
        "\n",
        "train_ds = configure(train_ds, train_size, augment=True)\n",
        "val_ds = configure(val_ds, val_size)\n",
        "test_ds = configure(test_ds, test_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Create version-controlled folder for weights file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = input(\"\"\"Create new folder?\n",
        "\n",
        "                (Y/n)    \n",
        "                \"\"\")\n",
        "\n",
        "new_version, path, save_path = generate_version('../Model/aiseries', version.lower() == 'y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Build a model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNxHjoEeb8TB"
      },
      "outputs": [],
      "source": [
        "%% write_and_run {path}/model.py\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "import os\n",
        "\n",
        "def create_model(path_to_weights=None, load_weights=True):\n",
        "    \"\"\"Function to create a model\n",
        "\n",
        "    Returns a compiled and optionally loaded model\n",
        "\n",
        "    Keyword arguments:\n",
        "\n",
        "    path_to_weights -- (Optional, only used when load_weights is True) -- Path to weight file (.hdf5 files)\n",
        "\n",
        "    load_weights -- Whether to load weights or not (default to True)\n",
        "    \"\"\"\n",
        "\n",
        "    assert(load_weights == True and \n",
        "           path_to_weights is not None and \n",
        "           os.path.isfile(path_to_weights)), \"path_to_weights must exist and not be NoneType if load_weights is True, otherwise change load_weights to False\"\n",
        "\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Input((32,32,1)),\n",
        "        keras.layers.Conv2D(64, (3,3), padding='same',\n",
        "                            kernel_regularizer=regularizers.l2(1e-3),\n",
        "                            activity_regularizer=regularizers.l2(1e-4),\n",
        "                            kernel_initializer='he_normal',\n",
        "                            activation='elu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Conv2D(64, (3,3), padding='same',\n",
        "                            kernel_regularizer=regularizers.l2(1e-3),\n",
        "                            activity_regularizer=regularizers.l2(1e-4),\n",
        "                            kernel_initializer='he_normal',\n",
        "                            activation='elu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "        keras.layers.Conv2D(128, (3,3), padding='same',\n",
        "                            kernel_regularizer=regularizers.l2(1e-3),\n",
        "                            activity_regularizer=regularizers.l2(1e-4),\n",
        "                            kernel_initializer='he_normal',\n",
        "                            activation='elu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Conv2D(128, (3,3), padding='same',\n",
        "                            kernel_regularizer=regularizers.l2(1e-3),\n",
        "                            activity_regularizer=regularizers.l2(1e-4),\n",
        "                            kernel_initializer='he_normal',\n",
        "                            activation='elu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "        keras.layers.Conv2D(256, (1,1), padding='same',\n",
        "                            kernel_regularizer=regularizers.l2(1e-3),\n",
        "                            activity_regularizer=regularizers.l2(1e-4),\n",
        "                            kernel_initializer='he_normal',\n",
        "                            activation='elu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Conv2D(256, (1,1), padding='same',\n",
        "                            kernel_regularizer=regularizers.l2(1e-3),\n",
        "                            activity_regularizer=regularizers.l2(1e-4),\n",
        "                            kernel_initializer='he_normal',\n",
        "                            activation='elu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.AveragePooling2D(pool_size=(2,2)),\n",
        "\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(128, activation='elu',\n",
        "                        kernel_regularizer=regularizers.l2(1e-3),\n",
        "                        activity_regularizer=regularizers.l2(1e-4)),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(64, activation='elu',\n",
        "                        kernel_regularizer=regularizers.l2(1e-3),\n",
        "                        activity_regularizer=regularizers.l2(1e-4)),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    if load_weights:\n",
        "        model.load_weights(path_to_weights)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "                                                loss='sparse_categorical_crossentropy',\n",
        "                                                metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYXGttWEb89y",
        "outputId": "d5029a94-f07f-409d-d134-b0baa8cee9c3"
      },
      "outputs": [],
      "source": [
        "model = create_model(load_weights=False)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training session**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Design a callback to stop training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V5szfyncBDI"
      },
      "outputs": [],
      "source": [
        "class stopCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}) :\n",
        "        if (logs.get('accuracy') >= 0.999 and \n",
        "            logs.get('val_accuracy') >= 0.999) :\n",
        "            print('\\nReached 99.9% accuracy so stopping training')\n",
        "            self.model.stop_training = True\n",
        "\n",
        "callback = stopCallback()\n",
        "\n",
        "#callback to save weights with the minimum loss value\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
        "                                                               monitor='val_loss',\n",
        "                                                               mode='min',\n",
        "                                                               save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vTAeSb1cBwn",
        "outputId": "42b83134-f833-4e5b-ee2b-90803624b22f"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds, \n",
        "                    epochs=50, \n",
        "                    callbacks=[callback, model_checkpoint_callback], \n",
        "                    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Model evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on the test set using the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('../Model/aiseries/version1.hdf5')\n",
        "\n",
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate based on training's metrics history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Based on loss value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_epochs = len(history.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwplY4tbcDpI"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], color='red', label='Train loss')\n",
        "plt.plot(history.history['val_loss'], color='blue', label='Validation loss')\n",
        "\n",
        "plt.xticks(np.arange(number_of_epochs, step=4))\n",
        "plt.ylim((0, 1))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Based on accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'], color='red', label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], color='blue', label='Validation accuracy')\n",
        "\n",
        "plt.xticks(np.arange(number_of_epochs, step=4))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate with Confusion Matrix and Classification Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate actual and predicted value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true = np.concatenate([y for _, y in test_ds], axis=0)\n",
        "\n",
        "Y_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(Y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
        "                                        display_labels=[class_name.capitalize() for class_name in class_names],\n",
        "                                        cmap='Blues')\n",
        "\n",
        "plt.xticks(rotation=60)\n",
        "plt.savefig('../TrainingReport/confusion_matrix.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_rep = classification_report(y_true, y_pred, \n",
        "                                target_names=[class_name.capitalize() for class_name in class_names], \n",
        "                                output_dict=True)\n",
        "\n",
        "sn.heatmap(pd.DataFrame(clf_rep).iloc[:-1,:].T, annot=True)\n",
        "plt.savefig('../TrainingReport/report.pdf')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI Series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
